{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:55.179117Z",
     "start_time": "2022-10-26T03:03:46.990209Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:55.383403Z",
     "start_time": "2022-10-26T03:03:55.181116Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:56.501483Z",
     "start_time": "2022-10-26T03:03:55.385285Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\"\"\"Z-score标准化输入使其平均数为0，标准差为1，\n",
    "    加快梯度下降求最优解速度，可能可以提高精度\"\"\"\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)#平均数\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)#标准差\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 幂调度（Power Scheduling）:学习率开始时迅速下降，后来越来越慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-23T02:15:47.189858Z",
     "start_time": "2022-10-23T02:15:47.177828Z"
    }
   },
   "source": [
    "#### lr = lr0 / (1 + steps / s)**c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:56.517459Z",
     "start_time": "2022-10-26T03:03:56.503460Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keras uses c=1 and s = 1 / decay\n",
    "#s：步骤，t：迭代次数\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指数调度（Exponential Scheduling）：每s步逐渐下降10倍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lr = lr0 * 0.1**(epoch / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:56.533494Z",
     "start_time": "2022-10-26T03:03:56.520456Z"
    }
   },
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    #lr0为初始学习率\n",
    "    \n",
    "    def exponential_decay_fn(epoch):\n",
    "        #采用当前轮次返回学习率\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    \n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LearningRateScheduler（）在每个轮次开始时更新优化器的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 若想更频繁的更新学习率（当每个轮次有很多步骤），可以自己编写回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.725725Z",
     "start_time": "2022-10-26T03:03:56.535454Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "n_epochs = 25\n",
    "\n",
    "\"\"\"将学习率传递给LearningRateScheduler回调函数，\n",
    "    再将回调函数传递给fit（）\"\"\"\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    " #                   validation_data=(X_valid_scaled, y_valid),\n",
    "  #                  callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分段恒定调度（Piecewise Constant Scheduling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.741705Z",
     "start_time": "2022-10-26T03:03:57.727704Z"
    }
   },
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.757418Z",
     "start_time": "2022-10-26T03:03:57.743705Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"更通用\"\"\"\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    #argmax（）返回最大值的索引\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.803454Z",
     "start_time": "2022-10-26T03:03:57.759418Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"将学习率传递给LearningRateScheduler回调函数，\n",
    "    再将回调函数传递给fit（）\"\"\"\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    " #                   validation_data=(X_valid_scaled, y_valid),\n",
    "  #                  callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能调度（Performance Scheduling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.865414Z",
     "start_time": "2022-10-26T03:03:57.805416Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "#使用ReduceLROnPlateau回调函数，当5个轮次最好验证损失没有改善则将学习率*0.5\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "          #          validation_data=(X_valid_scaled, y_valid),\n",
    "           #         callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras schedulers：每个步骤更新学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.929426Z",
     "start_time": "2022-10-26T03:03:57.867414Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "#python//表示取整除 ，返回商的整数部分（向下取整）\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "#使用指数衰减的学习率\n",
    "#初始值是0.01，梯度为s，衰减率（底数）为0.1，每s步学习率*0.1\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "           #         validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.945413Z",
     "start_time": "2022-10-26T03:03:57.930414Z"
    }
   },
   "outputs": [],
   "source": [
    "#分段恒定调度的学习率，每个步骤更新\n",
    "n_steps_per_epoch = 10#每个轮次的步骤数\n",
    "\n",
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1周期方法（1Cycle scheduling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.961414Z",
     "start_time": "2022-10-26T03:03:57.947426Z"
    }
   },
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "#后端\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    \"\"\"根据系数factor指数调度学习率，\n",
    "    使学习率与factor相乘，进行线性变换\"\"\"\n",
    "    \n",
    "    def __init__(self, factor):\n",
    "        \"\"\"每个与类相关联的方法调用都自动传递实参self\n",
    "           以self为前缀的变量可以供类中的所有办法使用\"\"\"\n",
    "        \n",
    "        self.factor = factor #系数\n",
    "        #获取存储在形参factor中的值并存储到变量factor中\n",
    "        \n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        \"\"\"在每个批次处理的最后，保存模型的lr和loss，\n",
    "           对模型学习率进行线性增长（×factor）\"\"\"\n",
    "        \n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        #将模型优化器的学习率参数添加进（存储在）rates数组\n",
    "        \n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        #将日志中计算出的loss保存至losses数组\n",
    "        \n",
    "        K.set_value(self.model.optimizer.learning_rate,\n",
    "                    self.model.optimizer.learning_rate * self.factor)\n",
    "        #tf.keras.backend.set_value(x,value)，从Numpy数组将值载入tensor中\n",
    "        #x：要设置为新值的Tensor\n",
    "        #value：Numpy数组（具有相同形状），将载入x\n",
    "        #将模型的学习率设置为：与系数factor相乘后的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.977419Z",
     "start_time": "2022-10-26T03:03:57.964420Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32,\n",
    "                       min_rate=10**-5, max_rate=10):\n",
    "    \"\"\"找到最优学习率，设置为最大学习率，\n",
    "       初始最小学习率设置为10的-5次方，最大学习率设置为10，\n",
    "       在此范围内寻找最优学习率\"\"\"\n",
    "    \n",
    "    init_weights = model.get_weights()#获得模型原始参数\n",
    "    \n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    #学习率总迭代次数\n",
    "    #lr在处理每个batch批次的最后和每个epoch的最后更新\n",
    "    #math.ceil(x) 方法将 x 向上舍入到最接近的整数\n",
    "    \n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    #系数\"\"\"按需设置\"\"\"\n",
    "    #numpy.exp()：返回e的幂次方，e是一个常数为2.71828\n",
    "    \n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    #得到模型原始学习率init_lr\n",
    "    \n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    #将模型学习率设置为最小学习率min_rate，\n",
    "    \n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    #在每个轮次训练的最后lr都与factor相乘在训练中途线性增长\n",
    "    #每个轮次都保存lr到rates数组，保存loss到losses数组\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    #训练模型\n",
    "    \n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    #将模型学习率设置为初始学习率init_lr\n",
    "    \n",
    "    model.set_weights(init_weights)\n",
    "    #将模型权重设置为初始权重\n",
    "    \n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "    #返回记录了每个批次变化的学习率和loss的rates[]和losses[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:03:57.993416Z",
     "start_time": "2022-10-26T03:03:57.979418Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#使得图片可见\n",
    "import matplotlib.pylab as plt#导入绘图库\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:04:53.161484Z",
     "start_time": "2022-10-26T03:04:51.031396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.3669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1e-05, 9.683906, 1.650153398513794, 3.204094512122018]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train,\n",
    "                                   epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "\"\"\"找到图中损失值降低的最快的点对应的学习率，确定最优学习率\"\"\"\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:04:00.409023Z",
     "start_time": "2022-10-26T03:04:00.395023Z"
    }
   },
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"训练开始到中途从 lr0（0.005）到lr1（0.05），\n",
    "    训练的后半部分降回lr0（0.005），\n",
    "    最后的几个轮次再往下降到1.0663265306122723e-05\"\"\"\n",
    "\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        \n",
    "        self.iterations = iterations\n",
    "        #学习率迭代次数#print(iterations) #设置为8800\n",
    "        \n",
    "        self.max_rate = max_rate \n",
    "        #学习率最大值#print(max_rate)  #设置为0.05\n",
    "        \n",
    "        self.start_rate = start_rate or max_rate / 10 #初始学习率\n",
    "        #最大学习率大约比初始学习率大10倍 #print(self.start_rate)#0.005\n",
    "        \n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        #最后几个轮次学习率的迭代次数\n",
    "        #lr0-lr1和lr1-lr0各占据学习率迭代的约一半，最后lr0降低几个数量级相当于1/11\n",
    "        #//为整除 #print(self.last_iterations) #881\n",
    "        \n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        #训练到中途学习率的迭代次数\n",
    "        #print(self.half_iteration) #3959 #print(2 * self.half_iteration)#7918\n",
    "        \n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        #最后几个轮次的 学习率\n",
    "        #print(self.last_rate) #5e-06\n",
    "        \n",
    "        self.iteration = 0\n",
    "        #将当前学习率迭代次数设置为0\n",
    "        \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        #插值\n",
    "        \"\"\"学习率迭代次数从iter1到iter2的过程中，学习率从rate1线性变化到rate2\"\"\"\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)/ (iter2 - iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        #在每个批次开始时\n",
    "        \n",
    "        if self.iteration < self.half_iteration:\n",
    "            #第一阶段，lr0-lr1\n",
    "            rate = self._interpolate(0, self.half_iteration, \n",
    "                                     self.start_rate, self.max_rate)\n",
    "            \n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            #第二阶段，lr1-lr2\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "            \n",
    "        else:\n",
    "            #第三阶段，最后几个批次学习率降低几个数量级\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "              \n",
    "        self.iteration += 1\n",
    "        #当前学习率迭代次数＋1\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, rate)\n",
    "        #将变化的学习率应用于模型优化器的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:04:34.254475Z",
     "start_time": "2022-10-26T03:04:00.411022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 2s 4ms/step - loss: 0.6544 - accuracy: 0.7763 - val_loss: 0.4773 - val_accuracy: 0.8370\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4528 - accuracy: 0.8424 - val_loss: 0.4282 - val_accuracy: 0.8530\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.4096 - accuracy: 0.8566 - val_loss: 0.4060 - val_accuracy: 0.8578\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3822 - accuracy: 0.8655 - val_loss: 0.3909 - val_accuracy: 0.8646\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3600 - accuracy: 0.8726 - val_loss: 0.3856 - val_accuracy: 0.8660\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3433 - accuracy: 0.8786 - val_loss: 0.3682 - val_accuracy: 0.8762\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3272 - accuracy: 0.8830 - val_loss: 0.3599 - val_accuracy: 0.8702\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3139 - accuracy: 0.8887 - val_loss: 0.3468 - val_accuracy: 0.8762\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.3024 - accuracy: 0.8902 - val_loss: 0.3455 - val_accuracy: 0.8752\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.8955 - val_loss: 0.3559 - val_accuracy: 0.8724\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2817 - accuracy: 0.8981 - val_loss: 0.3398 - val_accuracy: 0.8810\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2673 - accuracy: 0.9019 - val_loss: 0.3681 - val_accuracy: 0.8644\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2503 - accuracy: 0.9095 - val_loss: 0.3277 - val_accuracy: 0.8860\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2359 - accuracy: 0.9148 - val_loss: 0.3303 - val_accuracy: 0.8848\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2253 - accuracy: 0.9189 - val_loss: 0.3226 - val_accuracy: 0.8890\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2131 - accuracy: 0.9231 - val_loss: 0.3196 - val_accuracy: 0.8900\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.2037 - accuracy: 0.9267 - val_loss: 0.3194 - val_accuracy: 0.8896\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1947 - accuracy: 0.9309 - val_loss: 0.3223 - val_accuracy: 0.8898\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1865 - accuracy: 0.9342 - val_loss: 0.3230 - val_accuracy: 0.8896\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1793 - accuracy: 0.9373 - val_loss: 0.3139 - val_accuracy: 0.8928\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1731 - accuracy: 0.9398 - val_loss: 0.3134 - val_accuracy: 0.8938\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1675 - accuracy: 0.9417 - val_loss: 0.3125 - val_accuracy: 0.8928\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1631 - accuracy: 0.9443 - val_loss: 0.3130 - val_accuracy: 0.8934\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 2s 4ms/step - loss: 0.1604 - accuracy: 0.9452 - val_loss: 0.3114 - val_accuracy: 0.8952\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9463 - val_loss: 0.3115 - val_accuracy: 0.8950\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, \n",
    "                             max_rate=0.05)\n",
    "#传入学习率迭代次数和最大学习率\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
