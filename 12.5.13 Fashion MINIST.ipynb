{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VIgGySUbN_-5"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import time\n","from tqdm import trange\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEGTa58C67Ps"},"outputs":[],"source":["(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","X_train_full = X_train_full.astype(np.float32) / 255.\n","X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n","y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n","X_test = X_test.astype(np.float32) / 255."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1670081824902,"user":{"displayName":"salad xx","userId":"13786222985564916523"},"user_tz":-480},"id":"1e0z63TWP1LW","outputId":"7a6ad9a3-5ded-451d-8843-0605d1b93f27"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train: (55000, 28, 28)\n","y_train: (55000,)\n","X_test: (10000, 28, 28)\n","y_test: (10000,)\n","X_valid: (5000, 28, 28)\n","y_valid (5000,)\n"]}],"source":["print(\"X_train:\",X_train.shape)\n","print(\"y_train:\",y_train.shape)\n","print(\"X_test:\",X_test.shape)\n","print(\"y_test:\",y_test.shape)\n","print(\"X_valid:\",X_valid.shape)\n","print(\"y_valid\",y_valid.shape)"]},{"cell_type":"markdown","metadata":{"id":"NMynO4TlOOu1"},"source":["# 自定义训练循环"]},{"cell_type":"markdown","metadata":{"id":"ZFDEtS_i6ywx"},"source":["### a.\n","Exercise: Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOom3Gfv60lS"},"outputs":[],"source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","model = keras.models.Sequential([\n","    keras.layers.Flatten(input_shape=[28, 28]),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","    keras.layers.Dense(10, activation=\"softmax\"),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtzPO0b47JDs"},"outputs":[],"source":["n_epochs = 5\n","batch_size = 32\n","n_steps = len(X_train) // batch_size #迭代次数Iterations\n","# model.compile(loss=\"sparse_categorical_crossentropy\",\n","#               optimizer=\"sgd\",\n","#               metrics=[\"accuracy\"])\n","\n","#多分类任务\n","optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n","loss_fn = keras.losses.sparse_categorical_crossentropy\n","mean_loss = keras.metrics.Mean()\n","metrics = [keras.metrics.SparseCategoricalAccuracy()]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670081824903,"user":{"displayName":"salad xx","userId":"13786222985564916523"},"user_tz":-480},"id":"5i-ge7F_mKsJ","outputId":"86736ae7-e587-4222-897a-5b3d04879899"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1718"]},"metadata":{},"execution_count":6}],"source":["n_steps"]},{"cell_type":"markdown","metadata":{"id":"X9rkquSXighn"},"source":["np.random.randint(low，high=None，size=None,dtype)：返回随机整型数组，范围为[low,high),若high未填写则[0，low)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRJcAWfDSXwz"},"outputs":[],"source":["def random_batch(X, y, batch_size=32):\n","    \"\"\"从训练集中随机采样一个batch的实例\"\"\"\n","    idx = np.random.randint(len(X), size=batch_size)\n","    return X[idx], y[idx]"]},{"cell_type":"markdown","metadata":{"id":"pBh3A7nff_uT"},"source":["{:.4f}：格式化小数点后四位数字的浮点数，使用回车\\r和end=\"\"却白状态栏始终打印在同一行上"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz2LOUJNSX0F"},"outputs":[],"source":["def print_status_bar(iteration, total,loss,metrics):\n","    \"\"\"\n","    显示训练状态，包括步数、步总数、从轮次开始以来的平均损失、其他指标，\n","    \"\"\"\n","    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n","                         for m in [loss] + (metrics or [])])\n","    end = \"\" if iteration < total else \"\\n\"\n","    #当迭代次数未达到总迭代次数时end=空，达到时结束（换一行）\n","    print(\"\\r{}/{} - \".format(iteration, total) ,metrics,end=' ')"]},{"cell_type":"markdown","metadata":{"id":"b4AzvnwOomHT"},"source":["apply_gradients(grads_and_vars, name=None, skip_gradients_aggregation=False, **kwargs)，grads_and_vars：List of (gradient, variable) pairs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxai4FWv8d6Q","executionInfo":{"status":"ok","timestamp":1670082093623,"user_tz":-480,"elapsed":150368,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"87d20ec8-c671-4160-8ac6-0444e531da9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","54976/55000 -  mean: 0.3707 - sparse_categorical_accuracy: 0.8692  - val_loss: 0.4218\n"," - val_accuracy: 0.8610\n","Epoch 2/5\n","54976/55000 -  mean: 0.3620 - sparse_categorical_accuracy: 0.8690  - val_loss: 0.3991\n"," - val_accuracy: 0.8618\n","Epoch 3/5\n","54976/55000 -  mean: 0.3639 - sparse_categorical_accuracy: 0.8705  - val_loss: 0.4041\n"," - val_accuracy: 0.8664\n","Epoch 4/5\n","54976/55000 -  mean: 0.3602 - sparse_categorical_accuracy: 0.8722  - val_loss: 0.4198\n"," - val_accuracy: 0.8632\n","Epoch 5/5\n","54976/55000 -  mean: 0.3490 - sparse_categorical_accuracy: 0.8747  - val_loss: 0.4274\n"," - val_accuracy: 0.8648\n"]}],"source":["for epoch in range(1, n_epochs + 1):\n","    #该循环用于一代训练（每个轮次）（使用训练集的全部数据对模型进行一次完整训练）\n","  print(\"Epoch {}/{}\".format(epoch, n_epochs))\n","    \n","  for step in range(1, n_steps + 1):\n","        #该循环用于轮次内（每个epoch内部）的批（每个batch）处理\n","        \n","      X_batch, y_batch =random_batch(X_train, y_train)\n","        #从训练集中随机采样抽取一个batch的实例\n","      with tf.GradientTape() as tape:\n","          y_pred = model(X_batch)#使用模型作为函数，对一个批次进行预测\n","          main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","          #计算主要损失：每个实例的损失的均值\n","          loss = tf.add_n([main_loss] + model.losses)#加上每层都有的l2正则化损失\n","            \n","      gradients = tape.gradient(loss, model.trainable_variables)\n","        #针对每个可训练的变量计算损失的梯度\n","      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        #将每个可训练参数的元素打包成元组\n","        #用优化器执行梯度下降,更新可训练变量的参数\n","      for variable in model.variables:\n","          if variable.constraint is not None:\n","                #constraints 模块的函数允许在优化期间对网络参数设置约束（例如非负性）\n","                #当存在约束时，在梯度下降结束后应用\n","              variable.assign(variable.constraint(variable))\n","        \n","      loss=mean_loss(loss)#更新平均损失#mean_loss = keras.metrics.Mean(name=\"loss\")\n","      for metric in metrics:\n","          metric(y_batch, y_pred)\n","      #在轮次内显示状态栏，实时更新参数\n","      print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n","\n","  y_pred = model(X_valid)\n","  val_loss= np.mean(loss_fn(y_valid, y_pred))\n","  val_accuracy=np.mean(keras.metrics.sparse_categorical_accuracy(\n","                 tf.constant(y_valid, dtype=np.float32), y_pred))\n","  print(\" - val_loss: {:.4f}\".format(val_loss))\n","  print(\" - val_accuracy: {:.4f}\".format(val_accuracy))\n","    \n","  #重置平均损失和指标的状态\n","  for metric in [mean_loss] + metrics:\n","      metric.reset_states()"]},{"cell_type":"markdown","source":["## b.\n","Exercise: Try using a different optimizer with a different learning rate for the upper layers and the lower layers."],"metadata":{"id":"jwOnWxH8pL66"}},{"cell_type":"code","source":["keras.backend.clear_session()\n","np.random.seed(42)\n","tf.random.set_seed(42)"],"metadata":{"id":"9sJCJh5Rn4m-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ck76EKd2Smue"},"outputs":[],"source":["lower_layers = keras.models.Sequential([\n","    keras.layers.Flatten(input_shape=[28, 28]),\n","    keras.layers.Dense(100, activation=\"relu\"),\n","])\n","upper_layers = keras.models.Sequential([\n","    keras.layers.Dense(10, activation=\"softmax\"),\n","])\n","model = keras.models.Sequential([\n","    lower_layers, upper_layers\n","])\n","\n","lower_optimizer = keras.optimizers.SGD(learning_rate=1e-4)\n","upper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)"]},{"cell_type":"code","source":["n_epochs = 5\n","batch_size = 32\n","n_steps = len(X_train) // batch_size\n","loss_fn = keras.losses.sparse_categorical_crossentropy\n","mean_loss = keras.metrics.Mean()\n","metrics = [keras.metrics.SparseCategoricalAccuracy()]"],"metadata":{"id":"wuQIn47IoK1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1, n_epochs + 1):\n","    #该循环用于一代训练（每个轮次）（使用训练集的全部数据对模型进行一次完整训练）\n","  print(\"Epoch {}/{}\".format(epoch, n_epochs))\n","    \n","  for step in range(1, n_steps + 1):\n","        #该循环用于轮次内（每个epoch内部）的批（每个batch）处理\n","        \n","      X_batch, y_batch =random_batch(X_train, y_train)\n","        #从训练集中随机采样抽取一个batch的实例\n","      with tf.GradientTape() as tape:\n","          y_pred = model(X_batch)#使用模型作为函数，对一个批次进行预测\n","          main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","          #计算主要损失：每个实例的损失的均值\n","          loss = tf.add_n([main_loss] + model.losses)#加上每层都有的l2正则化损失\n","            \n","      gradients = tape.gradient(loss, model.trainable_variables)\n","        #针对每个可训练的变量计算损失的梯度\n","      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        #将每个可训练参数的元素打包成元组\n","        #用优化器执行梯度下降,更新可训练变量的参数\n","      for variable in model.variables:\n","          if variable.constraint is not None:\n","                #constraints 模块的函数允许在优化期间对网络参数设置约束（例如非负性）\n","                #当存在约束时，在梯度下降结束后应用\n","              variable.assign(variable.constraint(variable))\n","        \n","      loss=mean_loss(loss)#更新平均损失#mean_loss = keras.metrics.Mean(name=\"loss\")\n","      for metric in metrics:\n","          metric(y_batch, y_pred)\n","      #在轮次内显示状态栏，实时更新参数\n","      print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n","\n","  y_pred = model(X_valid)\n","  val_loss= np.mean(loss_fn(y_valid, y_pred))\n","  val_accuracy=np.mean(keras.metrics.sparse_categorical_accuracy(\n","                 tf.constant(y_valid, dtype=np.float32), y_pred))\n","  print(\" - val_loss: {:.4f}\".format(val_loss))\n","  print(\" - val_accuracy: {:.4f}\".format(val_accuracy))\n","    \n","  #重置平均损失和指标的状态\n","  for metric in [mean_loss] + metrics:\n","      metric.reset_states()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0NRlEFO4oayV","executionInfo":{"status":"ok","timestamp":1670082389549,"user_tz":-480,"elapsed":149044,"user":{"displayName":"salad xx","userId":"13786222985564916523"}},"outputId":"7a5ec46f-e27a-4685-ed2e-a5751cba5c57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","54976/55000 -  mean: 0.4524 - sparse_categorical_accuracy: 0.8430  - val_loss: 0.4866\n"," - val_accuracy: 0.8440\n","Epoch 2/5\n","54976/55000 -  mean: 0.4509 - sparse_categorical_accuracy: 0.8452  - val_loss: 0.4603\n"," - val_accuracy: 0.8494\n","Epoch 3/5\n","54976/55000 -  mean: 0.4315 - sparse_categorical_accuracy: 0.8491  - val_loss: 0.4676\n"," - val_accuracy: 0.8524\n","Epoch 4/5\n","54976/55000 -  mean: 0.4297 - sparse_categorical_accuracy: 0.8504  - val_loss: 0.4870\n"," - val_accuracy: 0.8506\n","Epoch 5/5\n","54976/55000 -  mean: 0.4185 - sparse_categorical_accuracy: 0.8550  - val_loss: 0.5129\n"," - val_accuracy: 0.8458\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mU9M0qH_pCGv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}